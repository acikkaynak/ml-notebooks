{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Notes by Ertugrul:\n",
    "# - FocalLoss\n",
    "# - Objective metric\n",
    "# - data_collator \n",
    "# - Hyperparameter seach\n",
    "# - Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "import optuna\n",
    "import random\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = int):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    return random_state\n",
    "\n",
    "\n",
    "random_state = set_seed(42)\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent = datasets.load_dataset(\"deprem-private/deprem_intent_classification\", \"intent_multilabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2ix = {'Alakasiz': 0, 'Barinma': 1, 'Elektronik': 2, 'Giysi': 3, 'Kurtarma': 4, 'Lojistik': 5, 'Saglik': 6, 'Su': 7, 'Yagma': 8, 'Yemek': 9}\n",
    "ix2name = {v: k for k, v in name2ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame().from_records(list(intent[\"train\"]))\n",
    "df_val = pd.DataFrame().from_records(list(intent[\"validation\"]))\n",
    "df_test = pd.DataFrame().from_records(list(intent[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_col = \"labels\"\n",
    "text_col = \"text_cleaned\"\n",
    "\n",
    "df_train = df_train[df_train[label_col].notnull()].reset_index(drop=True)\n",
    "df_val = df_val[df_val[label_col].notnull()].reset_index(drop=True)\n",
    "\n",
    "df_test = df_test[df_test[label_col].notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[1, \"text_cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=list(name2ix.values()))\n",
    "mlb_labels = mlb.fit_transform(df_train.labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = set()\n",
    "for label in df_train.labels.values:\n",
    "    labels.update({l for l in label})\n",
    "\n",
    "labels = list(sorted(labels))\n",
    "print(labels)\n",
    "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"dbmdz/bert-base-turkish-128k-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "\n",
    "  token_count = len(tokenizer.encode(\n",
    "\n",
    "    row[\"text\"],\n",
    "\n",
    "    max_length=128,\n",
    "\n",
    "    truncation=True\n",
    "\n",
    "  ))\n",
    "\n",
    "  token_counts.append(token_count)\n",
    "\n",
    "sns.histplot(token_counts)\n",
    "\n",
    "plt.xlim([0, 128]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, num_classes=len(labels)):\n",
    "        self.df = df\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text, label = row.text_cleaned, self._encode_label(row.labels)\n",
    "        encoding = tokenizer(text, max_length=128, truncation=True)\n",
    "        encoding = {key: torch.tensor(val, dtype=torch.int64) for key, val in encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(label, dtype=torch.float32)\n",
    "        return dict(encoding)\n",
    "    \n",
    "    def _encode_label(self, input_labels):\n",
    "        encoded_labels = np.zeros(self.num_classes)\n",
    "        for label in input_labels:\n",
    "            encoded_labels[label2idx[label]] = 1.0\n",
    "        return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = IntentDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           label2id=name2ix,\n",
    "                                                           id2label=ix2name                      \n",
    "                                                          ).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occs = np.sum(mlb_labels[df_train.index],\n",
    "       axis=0)\n",
    "\n",
    "occ_ratios = (mlb_labels.sum() / mlb_labels.sum(axis=0))\n",
    "occ_ratios /= occ_ratios.min()\n",
    "occ_ratios = np.power(occ_ratios, 1/3)\n",
    "\n",
    "class_weights = dict(zip(np.arange(mlb_labels.shape[1]), occ_ratios))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, pos_weight, alpha=0.1, gamma=2., reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight.to('cuda')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedTrainer(Trainer):\n",
    "    def __init__(self, inp_class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # You pass the class weights when instantiating the Trainer\n",
    "        self.class_weights = torch.Tensor(list(inp_class_weights.values())).cuda()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "\n",
    "            # Changes start here\n",
    "            # loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "            logits = outputs['logits']\n",
    "            criterion = FocalLoss(pos_weight=self.class_weights)\n",
    "            loss = criterion(logits, inputs['labels'])\n",
    "            # Changes end here\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = sigmoid(pred.predictions)\n",
    "    y_pred = (y_pred>0.5).astype(float)\n",
    "    clf_dict = classification_report(y_true, y_pred,\n",
    "    zero_division=0, output_dict=True)\n",
    "    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n",
    "\"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "step_size = int(np.ceil(len(df_train) / batch_size) / 4) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "HP_SEARCH = True\n",
    "\n",
    "if HP_SEARCH:\n",
    "    basic_args = TrainingArguments(\n",
    "        f\"turkish_multilabel_intent_{model_name.split('/')[-1]}\",\n",
    "        fp16=True,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"no\",\n",
    "        #learning_rate=2e-5,\n",
    "        #per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size*2,\n",
    "        #num_train_epochs=4,\n",
    "        #weight_decay=0.01,\n",
    "        load_best_model_at_end=False,\n",
    "         metric_for_best_model=\"macro f1\",\n",
    "        # eval_steps = step_size,\n",
    "        # save_steps = step_size,\n",
    "        # logging_steps = step_size,\n",
    "        seed = 42,\n",
    "        data_seed = 42,\n",
    "        dataloader_num_workers = 0,\n",
    "        #lr_scheduler_type = 'linear',\n",
    "        #warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
    "        #weight_decay=0,               # strength of weight decay\n",
    "        #save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "        full_determinism = True,\n",
    "        group_by_length = True\n",
    "    )\n",
    "\n",
    "    trainer = ImbalancedTrainer(\n",
    "        inp_class_weights=class_weights,                    \n",
    "        model_init=model_init,\n",
    "        args=basic_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=IntentDataset(df_train),\n",
    "        eval_dataset=IntentDataset(df_val),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    def hp_space(trial):\n",
    "        return {\n",
    "            \"num_train_epochs\" : trial.suggest_int(\"num_train_epochs\",2,5),\n",
    "            \"learning_rate\" : trial.suggest_float(\"learning_rate\", 1e-7, 1e-4),\n",
    "            \"weight_decay\" : trial.suggest_float(\"weight_decay\", 0.001, 0.1),\n",
    "            \"lr_scheduler_type\" : trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\"]),\n",
    "            \"per_device_train_batch_size\" : trial.suggest_int(\"per_device_train_batch_size\",8,32,8),\n",
    "            \"warmup_steps\" : trial.suggest_int(\"warmup_steps\",0,150,10),\n",
    "        }\n",
    "    \n",
    "\n",
    "    def compute_objective(metrics):\n",
    "        return metrics[\"eval_macro f1\"]\n",
    "\n",
    "    best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\", hp_space=hp_space, compute_objective=compute_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"turkish_multilabel_intent_{model_name.split('/')[-1]}\",\n",
    "    fp16=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    learning_rate=best_run[-1]['learning_rate'],\n",
    "    per_device_train_batch_size=best_run[-1]['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    num_train_epochs=4,\n",
    "    #weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "     metric_for_best_model=\"macro f1\",\n",
    "    eval_steps = step_size,\n",
    "    save_steps = step_size,\n",
    "    logging_steps = step_size,\n",
    "    seed = 42,\n",
    "    data_seed = 42,\n",
    "    dataloader_num_workers = 0,\n",
    "    lr_scheduler_type = best_run[-1]['lr_scheduler_type'],\n",
    "    warmup_steps=best_run[-1]['warmup_steps'],               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=best_run[-1]['weight_decay'],               # strength of weight decay\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    full_determinism = True,\n",
    "    group_by_length = True\n",
    ")\n",
    "\n",
    "trainer = ImbalancedTrainer(\n",
    "    inp_class_weights=class_weights,                    \n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=IntentDataset(df_train),\n",
    "    eval_dataset=IntentDataset(df_val),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(IntentDataset(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thr = -1\n",
    "best_score = 0.\n",
    "\n",
    "for threshold in np.arange(.1, 1., .03):\n",
    "    score = f1_score(preds.label_ids.astype(int), (sigmoid(preds.predictions) > threshold).astype(int), average=\"macro\")\n",
    "    if score>best_score:\n",
    "        best_score = score\n",
    "        best_thr = threshold\n",
    "\n",
    "best_thr, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(preds.label_ids.astype(int), (sigmoid(preds.predictions) > best_thr).astype(int), target_names=name2ix.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
